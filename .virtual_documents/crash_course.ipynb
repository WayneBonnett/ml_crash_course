





























import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns 
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import mean_squared_error, r2_score











# Generate random data
np.random.seed(0)  # For reproducibility
x = np.random.rand(100).reshape(-1, 1) # we have to reshape to make python happy 
y = 2 * x + 1 + np.random.randn(100).reshape(-1, 1) * 0.5

data = pd.DataFrame({'Weight (g) -> x': x.flatten(), 'Length (cm) -> y': y.flatten()}) # just for viewing our data
data.head()


# Plotting the points
plt.scatter(x, y)
plt.title("Earthworm Data")
plt.xlabel("Earthworm weight in g")
plt.ylabel("Earthworm length in cm")
plt.show()














# Split the data into training and testing sets 
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)


x_train[0:5]


y_train[0:5]


# we did an 80 / 20 split which is common 
print(f"Total size of x: {len(x)}\n size of x_train: {len(x_train)}\n size of x_test: {len(x_test)}")








# Fit the model 
model = LinearRegression() 
model.fit(x_train, y_train)





# Print the results 
print("Coefficients:", model.coef_) 
print("Intercept:", model.intercept_) 








# what if we want to do this with code? 
new_obs = np.array([[2]]) # our 2g worm 
pred = model.predict(new_obs)
pred[0][0]


# on more than one observation
# _pred for prediction is a common convention
y_pred = model.predict(x_test)

y_pred[0:5]











# Evaluate the model 
mse = mean_squared_error(y_test, y_pred) 
r2 = r2_score(y_test, y_pred) 

print("Mean Squared Error:", mse) 
print("R-squared:", r2)











from sklearn.linear_model import LogisticRegression 
from sklearn.metrics import accuracy_score


penguins = sns.load_dataset("penguins")
penguins.head()


# remove nulls, we want to predict sex. Might crash if we don't. 
penguins = penguins.dropna()

# Select features and target variable 
X = penguins.drop(columns=['bill_length_mm', 'bill_depth_mm', 'sex', 'species', 'island']) 
y = penguins['sex'] 

X.columns


# Split the data into training and testing sets 
# note the random_state argument, we want to be able to reproduce our work and this is randomly splitting the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 


# look at y_train, etc here if we want 
y_test


# Fit the Logistic Regression Model 
model = LogisticRegression(max_iter=200) 
model.fit(X_train, y_train) 


# Make predictions and evaluate the model 
y_pred = model.predict(X_test) 
accuracy = accuracy_score(y_test, y_pred) 
print(f'Accuracy: {accuracy * 100:.2f}%')





y_pred





X_2 = penguins.drop(columns=['sex', 'species', 'island']) 
y_2 = penguins['sex'] 

X_train, X_test, y_train, y_test = train_test_split(X_2, y_2, test_size=0.2, random_state=42) 

model_2 = LogisticRegression(max_iter=200) 
model_2.fit(X_train, y_train) 

y_pred = model_2.predict(X_test) 
accuracy = accuracy_score(y_test, y_pred) 
print(f'Accuracy: {accuracy * 100:.2f}%')








mtcars = sm.datasets.get_rdataset('mtcars', 'datasets').data
mtcars.head()


X = mtcars.drop(columns=['mpg']) 
y = mtcars['mpg']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# again, some math-y reasons but we have to scale the features 
# subtract mean, divide by standard deviation
scaler = StandardScaler() 
X_train = scaler.fit_transform(X_train) 
X_test = scaler.transform(X_test)

# fit model 
# we will tune the alpha parameter 
ridge = Ridge(alpha=1.0)  
ridge.fit(X_train, y_train)

# Make predictions on the test set 
y_pred = ridge.predict(X_test)

mse = mean_squared_error(y_test, y_pred) 
print(f'Mean Squared Error: {mse}')

# coeff, the betas 
print('Coefficients:', ridge.coef_)











# Define the model 
ridge = Ridge() 

# Define the grid of alpha values to search 
param_grid = {'alpha': [0.1, 1.0, 10.0, 100.0, 200.0]} 

# Use GridSearchCV to find the best alpha 
grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error') 
grid_search.fit(X_train, y_train) 

# Get the best alpha parameter 
best_alpha = grid_search.best_params_['alpha'] 
print(f'Best alpha: {best_alpha}') 

# Fit the ridge regression model with the best alpha on the entire training data 
best_ridge = Ridge(alpha=best_alpha) 
best_ridge.fit(X_train, y_train) 

# Make predictions on the test set 
y_pred = best_ridge.predict(X_test) 

# Evaluate the model 
mse = mean_squared_error(y_test, y_pred) 
print(f'Mean Squared Error: {mse}') 

# Display the coefficients
print('Coefficients:', best_ridge.coef_)


















