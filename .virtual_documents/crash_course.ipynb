

















import numpy as np 

A = np.array([[1, 2], [3, 4]])
A

















import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns 
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

from sklearn.metrics import mean_squared_error, r2_score











# Generate random data
np.random.seed(0)  # For reproducibility
x = np.random.rand(100).reshape(-1, 1) # we have to reshape to make python happy 
y = 2 * x + 1 + np.random.randn(100).reshape(-1, 1) * 0.5

data = pd.DataFrame({'Weight (g) -> x': x.flatten(), 'Length (cm) -> y': y.flatten()}) # just for viewing our data
data.head()


# Plotting the points
plt.scatter(x, y)
plt.title("Earthworm Data")
plt.xlabel("Earthworm weight in g")
plt.ylabel("Earthworm length in cm")
plt.show()





# find line of best fit
model = LinearRegression()
model.fit(x, y)
y_pred = model.predict(x)

# Plot data and line of best fit
plt.scatter(x, y, label='Data points')
plt.plot(x, y_pred, color='red', linewidth=2, label='Line of best fit')
plt.xlabel('Weight (g) -> x')
plt.ylabel('Length (cm) -> y')
plt.title('Scatter Plot with Line of Best Fit')
plt.legend()
plt.show()


















# Split the data into training and testing sets 
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)


x_train[0:5]


y_train[0:5]


# we did an 80 / 20 split which is common 
print(f"Total size of x: {len(x)}\n size of x_train: {len(x_train)}\n size of x_test: {len(x_test)}")








# Fit the model 
model = LinearRegression() 
model.fit(x_train, y_train)





# Print the results 
print("Coefficients:", model.coef_) 
print("Intercept:", model.intercept_) 








# what if we want to do this with code? 
new_obs = np.array([[2]]) # our 2g worm 
pred = model.predict(new_obs)
pred[0][0]


# on more than one observation
# _pred for prediction is a common convention
y_pred = model.predict(x_test)

y_pred[0:5]











# print these results out manually 
e = y_test - y_pred
n = e.shape[0] # this is me forgetting python syntax 
mse = (1/n) * (e**2).sum()
mse


# Evaluate the model with builtin functions 
mse = mean_squared_error(y_test, y_pred) 
r2 = r2_score(y_test, y_pred) 

print("Mean Squared Error:", mse) 
print("R-squared:", r2)




















from sklearn.linear_model import LogisticRegression 
from sklearn.metrics import accuracy_score


penguins = sns.load_dataset("penguins")
penguins.head()


# remove nulls, we want to predict sex. Might crash if we don't. 
penguins = penguins.dropna()

# Select features and target variable 
X = penguins.drop(columns=['bill_length_mm', 'bill_depth_mm', 'sex', 'species', 'island']) 
y = penguins['sex'] 

X.columns


# Split the data into training and testing sets 
# note the random_state argument, we want to be able to reproduce our work and this is randomly splitting the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 


# look at y_train, etc here if we want 
y_test


# Fit the Logistic Regression Model 
model = LogisticRegression(max_iter=200) 
model.fit(X_train, y_train) 


# Make predictions and evaluate the model 
y_pred = model.predict(X_test) 
accuracy = accuracy_score(y_test, y_pred) 
print(f'Accuracy: {accuracy * 100:.2f}%')





y_pred





X_2 = penguins.drop(columns=['sex', 'species', 'island']) 
y_2 = penguins['sex'] 

X_train, X_test, y_train, y_test = train_test_split(X_2, y_2, test_size=0.2, random_state=42) 

model_2 = LogisticRegression(max_iter=200) 
model_2.fit(X_train, y_train) 

y_pred = model_2.predict(X_test) 
accuracy = accuracy_score(y_test, y_pred) 
print(f'Accuracy: {accuracy * 100:.2f}%')











mtcars = sm.datasets.get_rdataset('mtcars', 'datasets').data
mtcars.head()


X = mtcars.drop(columns=['mpg']) 
y = mtcars['mpg']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# again, some math-y reasons but we have to scale the features 
# subtract mean, divide by standard deviation
scaler = StandardScaler() 
X_train = scaler.fit_transform(X_train) 
X_test = scaler.transform(X_test)

# fit model 
# we will tune the alpha parameter 
ridge = Ridge(alpha=1.0)  
ridge.fit(X_train, y_train)

# Make predictions on the test set 
y_pred = ridge.predict(X_test)

mse = mean_squared_error(y_test, y_pred) 
print(f'Mean Squared Error: {mse}')

# coeff, the betas 
print('Coefficients:', ridge.coef_)











# Define the model 
ridge = Ridge() 

# Define the grid of alpha values to search 
param_grid = {'alpha': [0.1, 1.0, 10.0, 100.0, 200.0]} 

# Use GridSearchCV to find the best alpha 
grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error') 
grid_search.fit(X_train, y_train) 

# Get the best alpha parameter 
best_alpha = grid_search.best_params_['alpha'] 
print(f'Best alpha: {best_alpha}') 

# Fit the ridge regression model with the best alpha on the entire training data 
best_ridge = Ridge(alpha=best_alpha) 
best_ridge.fit(X_train, y_train) 

# Make predictions on the test set 
y_pred = best_ridge.predict(X_test) 

# Evaluate the model 
mse = mean_squared_error(y_test, y_pred) 
print(f'Mean Squared Error: {mse}') 

# Display the coefficients
print('Coefficients:', best_ridge.coef_)











# Step 1: Import the Iris dataset
iris = load_iris()

# don't need this, but just so we can view it 
iris_view = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_view['target'] = iris.target 

iris_view


X = iris.data[:, :2]  # Use only the first two features (sepal length and sepal width)
y = iris.target

# Step 2: Perform K-means clustering
kmeans = KMeans(n_clusters=3, random_state=42)
y_kmeans = kmeans.fit_predict(X)

# Step 3: Plot the K-means clusters
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=20)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=50, c='red', label='Centroids')
plt.xlabel('Sepal Length')
plt.ylabel('Sepal Width')
plt.title('K-means Clustering on Iris Dataset (Sepal Dimensions)')
plt.legend()
plt.show()






species_names = iris.target_names

# Create a DataFrame for easier handling and plotting
df = pd.DataFrame(X, columns=['Sepal Length', 'Sepal Width'])
df['Species'] = [species_names[label] for label in y]

# Plot the sepal length and sepal width
plt.figure(figsize=(8, 6))

# Define colors and markers for each species
colors = ['blue', 'green', 'orange']
markers = ['o', 's', 'D']

for species, color, marker in zip(species_names, colors, markers):
    subset = df[df['Species'] == species]
    plt.scatter(subset['Sepal Length'], subset['Sepal Width'], c=color, label=species, marker=marker)

plt.xlabel('Sepal Length')
plt.ylabel('Sepal Width')
plt.title('Sepal Length vs. Sepal Width by Species')
plt.legend()
plt.show()












X = iris.data
y = iris.target
target_names = iris.target_names

# Apply PCA
pca = PCA(n_components=2)
X_r = pca.fit_transform(X)

# Create a DataFrame for easier visualization
df = pd.DataFrame(data=X_r, columns=['PC1', 'PC2'])
df['target'] = y
df['target_names'] = df['target'].apply(lambda i: target_names[i])

# Plot the PCA result
colors = ['navy', 'turquoise', 'darkorange']
plt.figure()
for color, target_name in zip(colors, target_names):
    plt.scatter(df[df['target_names'] == target_name]['PC1'], 
                df[df['target_names'] == target_name]['PC2'], 
                color=color, 
                alpha=.8, 
                lw=2,
                label=target_name)
plt.legend(loc='best', shadow=False, scatterpoints=1)
plt.title('PCA of Iris dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()







